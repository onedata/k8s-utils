elasticsearch:
  image: quay.io/pires/docker-elasticsearch-kubernetes:6.1.3
  client:
    replicas: 1
  master:
    replicas: 2
  nodeSelector:
    onedata.org/service.logs: ""
  tolerations:
    - key: onedata.org/service.logs
      effect: "PreferNoSchedule"
  data:
    replicas: 4

    ## Enable persistence using Persistent Volume Claims
    ## see: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: false

      ## Persistent volume storage class
      ##
      storageClass: default

      ## Persistent volume access mode
      ##
      accessMode: ReadWriteOnce

      ## Persistent volume size
      size: 12Gi

kibana:
  image: blacktop/kibana:6.1.3
  nodeSelector:
    onedata.org/service.logs: ""
  tolerations:
    - key: onedata.org/service.logs
      effect: "PreferNoSchedule"

fluentd:
  image: fluent/fluentd-kubernetes-daemonset:v0.12-debian-elasticsearch
  configFiles:
    fluent.conf: |
      @include kubernetes.conf
      # @include onedata.conf
      @include systemd.conf

      <match **>
        type elasticsearch
        log_level info
        include_tag_key true
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
        user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
        password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
        logstash_format true
        buffer_chunk_limit 2M
        buffer_queue_limit 32
        flush_interval 5s
        max_retry_wait 30
        disable_retry_limit
        num_threads 8
      </match>
    onedata.conf: |
      # Logs from systemd-journal for interesting services.
      # <source>
      #   @type tail
      #   filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      #   pos_file /var/log/fluentd-journald-kubelet.pos
      #   read_from_head true
      #   tag kubelet
      # </source>

      ## We use the concat plugin to handle multiline logs.
      ## We specify the regex of the starting of every OneData log, and concat next logs
      ## until we match the regex which means that we start a new log.
      <filter **>                               
        @type concat                            
        key log                                 
        multiline_start_regexp /^\[(op|oz)_(worker|panel)\] (\[\w )?\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d{3}/                                                                 
        flush_interval 5                        
        timeout_label @NORMAL                   
      </filter>

      ## We can use the rewrite_tag_filter plugin to categorize logs
      ## We match regex patterns to the logs, and if it matches we
      ## change the tag to include a category (ex: datastore, erlang, monitoring, ...)
      ## it is commented out for performance testing.
      ## We also use the relabel plugin to mark as @NORMAL logs that were timed out by the concat plugin (logs that are not multiline)
      #<match kubernetes.**>
      #  @type relabel
      #  @label @NORMAL
      #  @type rewrite_tag_filter
      #  rewriterule1 log dddaz\@changes\_worker od.datastore.${tag}
      #  rewriterule2 log CRASH\sREPORT od.erlang.${tag}
      #  rewriterule3 log Supervisor\scouchbeam od.datastore.${tag}
      #  rewriterule4 log gen\_server od.erlang.${tag}
      #  rewriterule5 log monitoring od.monitoring.${tag}
      #  rewriterule6 log DBSync od.dbsync.${tag}
      #  rewriterule7 log .+ od.untagged.${tag}
      #</match>

      ## As the code just over is commented out,
      ## we rewrite the relabel plugin. (see up for comment "We also use the relabel plugin...")
      <match **>
        @type relabel
        @label @NORMAL
      </match>

      <label @NORMAL>
      ##<match od.*>

      ## This filter is used with the rewrite_tag_filter, it allows to extract
      ## a part of the tag to put it in a new field (od_service).
      #<filter od.**>
      #  @type record_transformer
      ##  enable_ruby true
      #  <record>
      #    od_service ${tag_parts[1]}
      #  </record>
      #</filter>

      ## We match all logs starting with kubernetes.**, if we changed tags with the rewrite_tag_filter,
      ## we need to change the match parameters with "kubernetes.** od.**"
      <match kubernetes.**> #od.** kubernetes.**>
        @type copy
        <store>
      ## We store the data in the Elasticsearch instance
          type elasticsearch
          log_level info
          include_tag_key true
          host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
          port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
          scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
          user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
          password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
          # reload_connections true #"#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
          # logstash_prefix logstash #"#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
          logstash_format true
          buffer_chunk_limit 2M
          buffer_queue_limit 32
          flush_interval 5s
          max_retry_wait 30
          disable_retry_limit
          num_threads 8
        </store>
        <store>
          @type stdout
        </store>
      </match>
      </label>
    kubernetes.conf: |
      # Do not directly collect fluentd's own logs to avoid infinite loops.
      <match fluent.**>
        type null
      </match>

      # Example:
      # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
      <source>
        type tail
        path /var/log/containers/*.log
        pos_file /var/log/fluentd-containers.log.pos
        time_format %Y-%m-%dT%H:%M:%S.%NZ
        tag kubernetes.*
        format json
        read_from_head true
      </source>

      # <source>
      #   type tail
      #   format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      #   path /var/log/docker.log
      #   pos_file /var/log/fluentd-docker.log.pos
      #   tag docker
      # </source>

      # <source>
      #   type tail
      #   format none
      #   path /var/log/etcd.log
      #   pos_file /var/log/fluentd-etcd.log.pos
      #   tag etcd
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/kubelet.log
      #   pos_file /var/log/fluentd-kubelet.log.pos
      #   tag kubelet
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/kube-proxy.log
      #   pos_file /var/log/fluentd-kube-proxy.log.pos
      #   tag kube-proxy
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/kube-apiserver.log
      #   pos_file /var/log/fluentd-kube-apiserver.log.pos
      #   tag kube-apiserver
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/kube-controller-manager.log
      #   pos_file /var/log/fluentd-kube-controller-manager.log.pos
      #   tag kube-controller-manager
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/kube-scheduler.log
      #   pos_file /var/log/fluentd-kube-scheduler.log.pos
      #   tag kube-scheduler
      # </source>

      # <source>
      #   type tail
      #   format kubernetes
      #   multiline_flush_interval 5s
      #   path /var/log/glbc.log
      #   pos_file /var/log/fluentd-glbc.log.pos
      #   tag glbc
      # </source>

      <filter kubernetes.**>
        type kubernetes_metadata
      </filter>
    systemd.conf: |
      # Logs from systemd-journal for interesting services.
      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        pos_file /var/log/fluentd-journald-kubelet.pos
        read_from_head true
        tag kubelet
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "docker.service" }]
        pos_file /var/log/fluentd-journald-docker.pos
        read_from_head true
        tag docker
      </source>

      # <source>
      #   @type systemd
      #   filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      #   format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      #   path /var/log/docker.log
      #   pos_file /var/log/fluentd-docker.log.pos
      #   tag docker
      # </source>
